# Specification: Training Pipeline\n\n## Overview\nThis track implements the 'learning' capability of the hotword engine. It extends the existing neural network layers to support backpropagation and creates a pipeline to train models on custom WAV datasets.\n\n## Neural Network Gradients\n- **Backward Pass:** Every layer (Conv2D, ReLU, MaxPool2D, Dense) must implement a backward method to calculate gradients relative to its inputs and parameters.\n- **Loss Function:** Implement Binary Cross-Entropy (BCE) loss for hotword vs. background classification.\n\n## Optimization\n- **SGD:** A basic Stochastic Gradient Descent optimizer to update model weights based on calculated gradients.\n\n## Data Management\n- **Dataset Loader:** Logic to crawl directories (e.g., 'data/hotword' and 'data/background'), load WAVs, and convert them to Mel-Spectrograms on the fly.\n- **Augmentation:** Simple noise injection by mixing background samples into hotword samples during training.\n\n## Training Loop\n- **Iteration:** Logic to loop through epochs, perform forward/backward passes, and update weights.\n- **Validation:** Periodically check accuracy on a separate validation set.
