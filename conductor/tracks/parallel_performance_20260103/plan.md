# Plan: Performance Optimization (Multithreading & Parallelism)\n\n## Phase 1: Internal Loop Parallelism\n- [x] Task: Parallelize Conv2D and MaxPool2D (e35db24)\n  - [ ] Update `pkg/model/layers.go` to use goroutines for the filter/channel loops\n  - [ ] Ensure thread-safety for output buffer writes\n  - [ ] Write a benchmark to measure speedup on a large tensor\n- [x] Task: Parallelize Conv2DBackward and DenseBackward (e35db24)\n  - [ ] Use a similar approach for the backward pass loops\n- [ ] Task: Conductor - User Manual Verification 'Phase 1: Internal Loop Parallelism' (Protocol in workflow.md)\n\n## Phase 2: Data-Parallel Trainer\n- [ ] Task: Implement `ParallelTrainer` in `pkg/train/parallel.go`\n  - [ ] Implement dataset sharding logic\n  - [ ] Create worker loops that perform local training steps\n- [ ] Task: Implement Weight Averaging\n  - [ ] Write a function to average parameters across multiple `SequentialModel` instances\n  - [ ] Integrate synchronization at the end of each epoch\n- [ ] Task: Conductor - User Manual Verification 'Phase 2: Data-Parallel Trainer' (Protocol in workflow.md)\n\n## Phase 3: Integration and Benchmarking\n- [ ] Task: Update CLI and Configuration\n  - [ ] Add `threads` flag to `cmd/train.go` and `cmd/listen.go`\n  - [ ] Update `Trainer` initialization to use `ParallelTrainer` if threads > 1\n- [ ] Task: Performance Benchmarking\n  - [ ] Run a full training session on a dummy 1000-sample dataset and report speedup vs sequential\n- [ ] Task: Conductor - User Manual Verification 'Phase 3: Integration and Benchmarking' (Protocol in workflow.md)
