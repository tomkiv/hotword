# Specification: Deep Model Architecture (CNN)\n\n## Overview\nThis track replaces the simple single-layer Dense model with a flexible, multi-layer neural network architecture. It introduces a configuration-driven model definition and a robust binary format for storing parameters across multiple layers.\n\n## Functional Requirements\n- **Layer-based Model:** Implement a `Model` structure that manages a sequence of layers: `Conv2D`, `ReLU`, `MaxPool2D`, and `Dense`.\n- **Configuration Driven:** Allow users to define the model architecture in `config.yaml` (e.g., number of filters, kernel sizes, and units).\n- **Structured Binary Format:** Update the persistence logic to store layer metadata along with weights and biases, allowing the model to be rebuilt correctly upon loading.\n- **Unified Forward/Backward Pass:** Update the `Trainer` and `Engine` to iterate through the sequence of layers for training and inference.\n\n## Implementation Details\n- **Persistence:** Add a "Layer Count" and "Layer Type" header to the binary file.\n- **Math Engine:** Ensure all existing layers in `pkg/model/layers.go` are compatible with the new sequential model structure.\n- **Initialization:** Implement Xavier/Glorot initialization for all weight-bearing layers.\n\n## Acceptance Criteria\n- A CNN architecture can be successfully defined in `config.yaml` and trained.\n- The resulting model binary includes metadata for all layers.\n- The `listen` command works with the new multi-layer model without any code changes to the listening loop itself.\n- Training loss decreases faster or reaches a lower minimum compared to the single-layer Dense model on the same dataset.
