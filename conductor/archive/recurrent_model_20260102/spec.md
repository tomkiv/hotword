# Specification: Recurrent Model Support (GRU/LSTM)\n\n## Overview\nThis track solidifies and expands support for recurrent neural network architectures. It fixes a critical bug in parameter updates for multi-gate layers (like GRU), implements stateful inference for streaming audio, and adds LSTM as an additional recurrent layer option.\n\n## Functional Requirements\n- **Fixed Parameter Updates:** Update `pkg/train.Trainer` to ensure that layers returning copies of their weights (like GRU/LSTM) are correctly updated using `SetParams`.\n- **Stateful Inference:** Update `pkg/engine.Engine` to optionally maintain recurrent hidden states between `Process` calls, allowing the model to "remember" temporal patterns across audio chunks.\n- **LSTM Layer Implementation:** Add a new `LSTMLayer` to `pkg/model` with forward and backward pass (BPTT) support.\n- **Configuration & Persistence:**\n  - Update `BuildModelFromConfig` to support `lstm`.\n  - Ensure Version 2 binary persistence correctly saves and loads LSTM parameters.\n  - Update `config.yaml` to provide a sample GRU/LSTM architecture.\n\n## Implementation Details\n- **Trainer:** Always call `layer.SetParams()` after `SGDUpdate` to handle layers that concatenate internal weights for the interface.\n- **Engine:** Store a `hiddenState` slice in the `Engine` struct. If the model is recurrent, pass this state into the forward pass.\n- **LSTM:** Implement the standard 4-gate LSTM (input, forget, output, cell candidate) architecture.\n\n## Acceptance Criteria\n- A model using GRU or LSTM can be trained and saved successfully.\n- Training loss decreases significantly on a temporal dataset.\n- Real-time detection with a recurrent model is responsive and shows improved accuracy on sustained hotwords compared to the windowed CNN approach.\n- Loading an LSTM model from a binary file results in identical inference output compared to the original model.
