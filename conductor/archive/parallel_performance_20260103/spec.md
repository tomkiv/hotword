# Specification: Performance Optimization (Multithreading & Parallelism)\n\n## Overview\nThis track focuses on speeding up the training process by leveraging multi-core CPUs. We will implement Data-Parallel Sharding, allowing the trainer to utilize all available CPU cores by splitting the dataset and training multiple local model instances in parallel, with periodic weight averaging.\n\n## Functional Requirements\n- **Multithreaded Trainer:** Implement a `ParallelTrainer` that shards the dataset based on `runtime.NumCPU()`.\n- **Shard-based Training:** Each shard will have its own `Trainer` instance and local model copy.\n- **Weight Averaging:** Implement a synchronization step at the end of each epoch to average weights across all shard models and update the master model.\n- **Internal Loop Parallelism:** Parallelize the outermost filter loop in `Conv2D` and `Conv2DBackward` using a work pool or `sync.WaitGroup` to speed up both training and inference.\n- **Configuration:** Add a `num_threads` parameter to `config.yaml` and a `--threads` flag to the `train` command.\n\n## Implementation Details\n- **Data Sharding:** Divide the `Dataset.Samples` slice into `N` chunks.\n- **Synchronization:** Use `sync.WaitGroup` to wait for all shards to complete an epoch.\n- **Averaging Logic:** For each layer, sum the `weights.Data` from all shards and divide by `N`.\n- **Inference Speed:** Use `runtime.NumCPU()` goroutines in `Conv2D` to process filters in parallel.\n\n## Acceptance Criteria\n- Training on a large dataset (e.g., 1000+ samples) shows a near-linear speedup relative to the number of CPU cores.\n- The final averaged model achieves comparable accuracy to a model trained sequentially.\n- Inference latency for the `listen` command is reduced, especially for models with many filters.\n- `go test ./...` passes without race conditions (verify with `-race` flag).
